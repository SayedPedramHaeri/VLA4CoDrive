<div align="center">
  <img src="Images/VLA4CoDrivee.png" width="350"/>
  
  ## Visionâ€“Languageâ€“Action Dataset for Cooperative Autonomous Driving
</div>

<p align="justify">
<b>VLA4CoDrive</b> is a large-scale <b>cooperative Visionâ€“Languageâ€“Action (VLA)</b> dataset designed to support autonomous driving under <b>multi-vehicle cooperation</b>. The project provides synchronized multi-view perception, structured language grounding, and future trajectory/action supervision, enabling end-to-end learning of <b>perception, reasoning, and decision-making</b> in cooperative driving scenarios.
<br><br>
ğŸ† This work has been accepted to the <b>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</b>.
</p>

---

## ğŸ” Overview
<p align="justify">
We introduce VLA4CoDrive, a cooperative Visionâ€“Languageâ€“Action dataset with synchronized multi-vehicle sensing across diverse driving environments, providing multi-view visual streams, contextual text (caption, context, description, reasoning), and future trajectory actions for training and evaluating VLA driving models.

<div align="center">
  <img src="Images/VLA.jpg" width="1000"/>
