<div align="center">
  <img src="Images/VLA4CoDrivee.png" width="300"/>
  
  ## Vision–Language–Action Dataset for Cooperative Autonomous Driving
</div>

<p align="justify">
<b>VLA4CoDrive</b> is a large-scale <b>cooperative Vision–Language–Action (VLA)</b> dataset and research framework designed to support autonomous driving under <b>multi-vehicle cooperation</b>. The project provides synchronized multi-view perception, structured language grounding, and future trajectory/action supervision, enabling end-to-end learning of <b>perception, reasoning, and decision-making</b> in cooperative driving scenarios. Unlike prior datasets that treat each vehicle independently, VLA4CoDrive captures <b>two to three cooperating vehicles</b> operating within the same scene with strict frame-level alignment, allowing realistic modeling of <b>shared situational awareness and coordinated behavior</b>.
</p>
---
