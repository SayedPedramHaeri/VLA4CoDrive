<div align="center">
  <img src="Images/VLA4CoDrivee.png" width="350"/>
  
  ## Vision‚ÄìLanguage‚ÄìAction Dataset for Cooperative Autonomous Driving
  
[![paper](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/pdf/2109.07644.pdf)
[![Documentation Status](https://readthedocs.org/projects/opencood/badge/?version=latest)](https://opencood.readthedocs.io/en/latest/?badge=latest) 
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) 


  https://carla.readthedocs.io/en/latest/start_quickstart/
</div>

<p align="justify">
<b>VLA4CoDrive</b> is a large-scale <b>cooperative Vision‚ÄìLanguage‚ÄìAction (VLA)</b> dataset designed to support autonomous driving under multi-vehicle cooperation. This work has been accepted to the <b>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</b>.
</p>

---

## üîç Overview
<p align="justify">
We introduce VLA4CoDrive, a cooperative Vision‚ÄìLanguage‚ÄìAction dataset with synchronized multi-vehicle sensing across diverse driving environments, providing multi-view visual streams, contextual text (caption, context, description, reasoning), and future trajectory actions for training and evaluating VLA driving models.

<div align="center">
  <img src="Images/VLA.jpg" width="1000"/>
