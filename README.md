<div align="center">
  <img src="Images/VLA4CoDrivee.png" width="300"/>
  
  ## Vision–Language–Action Dataset for Cooperative Autonomous Driving
</div>

**VLA4CoDrive** is a large-scale **cooperative Vision–Language–Action (VLA)** dataset and research framework designed to support autonomous driving under **multi-vehicle cooperation**. The project provides synchronized multi-view perception, structured language grounding, and future trajectory/action supervision, enabling end-to-end learning of **perception, reasoning, and decision-making** in cooperative driving scenarios.

Unlike prior datasets that treat each vehicle independently, VLA4CoDrive captures **two to three cooperating vehicles** operating within the same scene with strict frame-level alignment, allowing realistic modeling of **shared situational awareness and coordinated behavior**.

---
